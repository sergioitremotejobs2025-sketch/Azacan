# Technical Guide: Libro-Mind E-commerce & AI System

## 1. Project Overview
**Libro-Mind** is a hybrid e-commerce platform that combines a traditional Django-based bookstore with an advanced AI-powered recommendation system. It leverages Retrieval-Augmented Generation (RAG) to provide semantic search and real-time streaming book recommendations.

---

## 2. System Architecture

The project follows a decoupled architecture containerized with Docker:

### 2.1 Technology Stack
- **Frontend Core**: Django Templates (Legacy Store) + Next.js 16.1 (Modern AI UI).
- **Backend**: Django 5.2 (Python 3.11).
- **Database**: PostgreSQL 15 with **pgvector** extension.
- **AI Inference Engine**: Ollama (running `deepseek-r1:1.5b`).
- **Embedding Model**: `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions).
- **DevOps**: Docker, Docker Compose.

### 2.2 Data Flow Diagram
1. **Query**: User enters a natural language query (e.g., "books about forbidden love").
2. **Embedding**: Django generates a vector for the query using SentenceTransformers.
3. **Retrieval**: pgvector performs a `CosineDistance` search against the `Book` table.
4. **Augmentation**: Top-K results are retrieved and formatted as context.
5. **Generation**: The context + query is sent to Ollama (`deepseek-r1`).
6. **Streaming**: LLM tokens are streamed back from Django -> Next.js Proxy -> Browser.

---

## 3. Core Components

### 3.1 RAG Engine (`recommendations/rag.py`)
Responsible for the "Brain" of the application.
- **Semantic Search**: Uses vector similarity to find books that match the *intent* of a query, not just keywords.
- **Prompt Engineering**: Uses structured prompts to force the LLM into specific output formats (JSON) while allowing creative reasoning.
- **Model Choice**: `deepseek-r1:1.5b` chosen for its superior instruction-following and low latency on consumer CPUs.

### 3.2 Streaming Architecture
- **Django**: Uses `StreamingHttpResponse` to yield generator chunks from the LLM.
- **Next.js**: Implements an API route (`/api/stream-recommendations`) that acts as a proxy, maintaining the stream connection and handling errors gracefully.
- **UI**: React-based `BookQueryForm.tsx` uses a `ReadableStream` reader to update the UI in real-time as tokens arrive.

### 3.3 Data Sync System (`sync_books_to_products.py`)
Bridges the gap between the "Research" database (`Book` model) and the "Commerce" database (`Product` model).
- Matches books by `reference` ID.
- Automatically assigns books to a "Libros" category.
- Ensures the Shopping Cart can target valid `Product` IDs even when items are suggested via AI.

---

## 4. Database Schema

### 4.1 Key Models
- **Book (recommendations app)**:
    - `embedding`: `VectorField(384)` for RAG.
    - `reference`: Unique vendor ID.
    - `title`, `author`, `description`.
- **Product (store app)**:
    - Standard commerce fields: `price`, `is_sale`, `image`.
    - `reference`: Foreign key-like field used for synchronization.

---

## 5. Infrastructure & Docker

The project is orchestrated via `docker-compose-app/docker-compose.yml`:

- **db**: Custom pgvector image. Auto-loads `init-pgvector.sql` and `book_store_db_backup.sql` on first start.
- **backend**: Django dev server with volume mounts for live-code editing.
- **frontend**: Next.js production-build container with internal `.next` volume persistence.
- **json-server**: Mock service for user management/metadata.

---

## 6. Key Workflows

### 6.1 Rebuilding the Environment
```bash
docker-compose down --rmi all --volumes
docker-compose up -d --build
```
This purges everything and forces a fresh database restoration from the backup, ensuring environment parity.

### 6.2 Manual Data Sync
To sync new books from the AI catalog to the store:
```bash
docker-compose exec backend python manage.py sync_books_to_products
```

---

## 7. Performance Optimizations
1. **Model Quantization**: Uses 1.5b parameter model to fit in RAM/CPU constraints.
2. **Persistence**: The `.next` folder in the frontend is stored in an anonymous volume to prevent host-container conflicts and speed up subsequent starts.
3. **Indexing**: B-Tree indexes on `reference` and `name` for fast lookups; HNSW/IVF indexes can be added for vector fields if scale increases.

---
*Generated by Antigravity AI - February 2026*
